# -*- coding: utf-8 -*-
"""pomegranate-fruit-disease-detection-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7aHci9hoKlBi7gXyaSfCGQOPXtsEHoa

**CLEANING OF DATASET
*
"""

import os
import cv2
from imghdr import what

# Define the dataset directory
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Path to your dataset
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']  # Categories in the dataset

# Function to clean the dataset
def clean_dataset(data_dir, categories):
    for category in categories:
        category_path = os.path.join(data_dir, category)

        # Check if the category directory exists
        if not os.path.exists(category_path):
            print(f"Directory {category_path} does not exist. Skipping.")
            continue

        # Iterate through each file in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)

            # Check if the file is an image
            if what(img_path) is None:  # Checks if the file is a valid image type
                print(f"{img_name} is not a valid image. Removing.")
                os.remove(img_path)
                continue

            # Try to read the image with OpenCV
            image = cv2.imread(img_path)
            if image is None:
                print(f"Failed to load image {img_name}. Removing.")
                os.remove(img_path)
                continue

            # Optionally, remove images that don't meet size requirements
            height, width, _ = image.shape
            if height < 128 or width < 128:  # Example condition, adjust as needed
                print(f"Image {img_name} is too small ({width}x{height}). Removing.")
                os.remove(img_path)

            # Add any other cleaning criteria here (e.g., checking for duplicates)

# Call the function to clean the dataset
clean_dataset(data_dir, categories)

"""##FEATURE EXTRACTION##

"""

'''import os
import numpy as np
import cv2
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)

# Preprocess function to load, resize, and flatten the images
def preprocess_images(data_dir, categories, target_size):
    images = []
    labels = []

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = category  # Use category names as labels

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            if image is not None:
                # Resize image to target size (128x128)
                image = cv2.resize(image, target_size)
                # Flatten the image into a 1D array (for pixel-level feature extraction)
                image = image.flatten()
                # Append image and label
                images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    images = np.array(images)
    labels = np.array(labels)

    return images, labels

# Load and preprocess images
images, labels = preprocess_images(data_dir, categories, target_size)

# Feature scaling (standardization) for more efficient PCA
scaler = StandardScaler()
images_scaled = scaler.fit_transform(images)

# Apply PCA to reduce dimensionality (optional step for feature extraction)
pca = PCA(n_components=100)  # Extract 100 principal components (features)
images_pca = pca.fit_transform(images_scaled)

# Print out the shape of the extracted features
print(f'Original feature shape (flattened): {images.shape}')
print(f'Feature shape after PCA: {images_pca.shape}') '''

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import cv2
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Path to your dataset
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']  # Categories in the dataset
target_size = (128, 128)

# Preprocess function to load, resize, and flatten the images
def preprocess_images(data_dir, categories, target_size):
    images = []
    labels = []

    for category in categories:
        category_path = os.path.join(data_dir, category)

        # Check if the category directory exists and skip if not
        if not os.path.exists(category_path):
            print(f"Directory {category_path} does not exist. Skipping.")
            continue

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            if image is not None:
                # Resize image to target size (128x128)
                image = cv2.resize(image, target_size)
                # Flatten the image into a 1D array (for pixel-level feature extraction)
                image = image.flatten()
                # Append image and label
                images.append(image)
                labels.append(category) # Use category as label
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    images = np.array(images)
    labels = np.array(labels)

    return images, labels

# Ensure Google Drive is mounted
# from google.colab import drive
# drive.mount('/content/drive')

# Load and preprocess images
images, labels = preprocess_images(data_dir, categories, target_size)

# Feature scaling (standardization) for more efficient PCA
scaler = StandardScaler()
images_scaled = scaler.fit_transform(images)

# Apply PCA to reduce dimensionality (optional step for feature extraction)
pca = PCA(n_components=100)  # Extract 100 principal components (features)
images_pca = pca.fit_transform(images_scaled)

# Print out the shape of the extracted features
print(f'Original feature shape (flattened): {images.shape}')
print(f'Feature shape after PCA: {images_pca.shape}')

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)

# Preprocess function to load, resize, and flatten the images
def preprocess_images(data_dir, categories, target_size):
    images = []
    labels = []

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = category

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            if image is not None:
                # Resize image to target size (128x128)
                image = cv2.resize(image, target_size)
                # Append image and label
                images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    images = np.array(images)
    labels = np.array(labels)

    return images, labels

# Load and preprocess images
images, labels = preprocess_images(data_dir, categories, target_size)

# Display some original images
def display_images(images, title="Original Images", num_images=5):
    plt.figure(figsize=(10, 5))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))
        plt.axis('off')
    plt.suptitle(title)
    plt.show()

# Display the original images
display_images(images)

# Flatten images for feature extraction (if needed for KNN, etc.)
images_flattened = [img.flatten() for img in images]

# Feature scaling for PCA
scaler = StandardScaler()
images_scaled = scaler.fit_transform(images_flattened)

# Apply PCA for dimensionality reduction (optional step for feature extraction)
pca = PCA(n_components=100)  # Extract 100 principal components
images_pca = pca.fit_transform(images_scaled)

# Inverse PCA to reconstruct the images from PCA features
images_pca_reconstructed = pca.inverse_transform(images_pca)
images_pca_reconstructed = scaler.inverse_transform(images_pca_reconstructed)

# Reshape flattened images back to 128x128x3 for visualization
images_pca_reconstructed = images_pca_reconstructed.reshape(-1, 128, 128, 3)

# Display PCA-reconstructed images
def display_pca_reconstructed(images_pca_reconstructed, title="PCA-Reconstructed Images", num_images=5):
    plt.figure(figsize=(10, 5))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        # Clip values to [0, 255] and convert to uint8 for display
        img = np.clip(images_pca_reconstructed[i], 0, 255).astype('uint8')
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.axis('off')
    plt.suptitle(title)
    plt.show()

# Display the PCA-reconstructed images
display_pca_reconstructed(images_pca_reconstructed)

import matplotlib.pyplot as plt
import cv2
import os
!pip install patool
import patoolib

# This is likely the directory containing the extracted files.
data_dir = "/content/drive/MyDrive/Pomegranate Diseases Dataset"
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']

# Unrar the file if the directory doesn't exist
if not os.path.exists(data_dir):
    patoolib.extract_archive("/content/drive/MyDrive/Pomegranate Diseases Dataset.rar", outdir="/content/drive/MyDrive")


def load_and_display_images(data_dir, categories, num_images=5):
    plt.figure(figsize=(15, 15))

    image_count = 1  # Counter to keep track of the subplot index

    # Loop through each category
    for category in categories:
        path = os.path.join(data_dir, category)  # Get the path for each class
        images = os.listdir(path)  # List all images in the category

        # Display up to 'num_images' images from each category
        for i in range(num_images):
            img_path = os.path.join(path, images[i])  # Get the path of the image
            image = cv2.imread(img_path)  # Read the image using OpenCV
            if image is not None:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV) to RGB (matplotlib)

                # Plot the image
                plt.subplot(len(categories), num_images, image_count)  # Set subplot position
                plt.imshow(image)  # Display the image
                plt.title(category)  # Set the title as the category name
                plt.axis('off')  # Turn off the axis labels
                image_count += 1  # Increment the counter for the next subplot

    plt.tight_layout()
    plt.show()

# Call the function to display images
load_and_display_images(data_dir, categories)

"""##HOG FEATURE EXTRACTION##"""

import os
import cv2
import numpy as np
from skimage.feature import hog
import matplotlib.pyplot as plt

# Define the dataset directory and target size for resizing
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Path to your dataset
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']  # Categories in the dataset
target_size = (128, 128)  # Resize all images to 128x128

# Function to preprocess images and extract HOG features
def extract_and_display_hog_images(data_dir, categories, target_size, images_per_class=10):
    for category in categories:
        category_path = os.path.join(data_dir, category)
        print(f"Displaying images from category: {category}")

        image_count = 0  # To track the number of images processed per category

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            if image_count >= images_per_class:
                break  # Stop after displaying 10 images per category

            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)

            if image is not None:
                # Resize the image
                image = cv2.resize(image, target_size)

                # Convert the image to grayscale (HOG works on grayscale images)
                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

                # Extract HOG features and HOG visualization
                hog_feature, hog_image = hog(
                    gray_image,
                    orientations=9,
                    pixels_per_cell=(8, 8),
                    cells_per_block=(2, 2),
                    block_norm='L2-Hys',
                    visualize=True,
                    feature_vector=True
                )

                # Display the original and HOG image
                plt.figure(figsize=(8, 4))
                plt.subplot(1, 2, 1)
                plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
                plt.title(f"Original - {category}")
                plt.axis('off')

                plt.subplot(1, 2, 2)
                plt.imshow(hog_image, cmap='gray')
                plt.title("HOG Visualization")
                plt.axis('off')

                plt.show()

                image_count += 1  # Increment image count
            else:
                print(f'Failed to process image: {img_path}')

# Call the function to display 10 images and HOG visualizations per class
extract_and_display_hog_images(data_dir, categories, target_size, images_per_class=10)

"""##DISEASEWISE ACCURACY##

"""

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns

# Example labels and predictions (replace these with your actual data)
labels = np.array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4]) # Example true labels
predictions = np.array([0, 1, 1, 2, 4, 0, 1, 2, 3, 4]) # Example predicted labels

# Calculate the confusion matrix
cm = confusion_matrix(labels, predictions)

# Calculate accuracy for each class
accuracy_per_class = cm.diagonal() / cm.sum(axis=1)

# Define categories
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']

# Create a bar plot for accuracy per class
plt.figure(figsize=(10, 6))
sns.barplot(x=categories, y=accuracy_per_class)
plt.ylim(0, 1)
plt.title('Disease-wise Accuracy')
plt.xlabel('Diseases')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.show()

"""##COLOR##"""

import cv2
import os
import numpy as np
import matplotlib.pyplot as plt

def extract_color_histogram(image):
    # Calculate histogram for each color channel
    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
    hist = cv2.normalize(hist, hist).flatten()
    return hist

# Example usage
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']

for category in categories:
    category_path = os.path.join(data_dir, category)
    for img_name in os.listdir(category_path):
        img_path = os.path.join(category_path, img_name)
        image = cv2.imread(img_path)
        if image is not None:
            hist = extract_color_histogram(image)
            print(f'Color Histogram for {img_name}: {hist}')

"""##contour feature##"""

def extract_contour_features(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Assuming the largest contour is the object of interest
    if contours:
        largest_contour = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(largest_contour)
        perimeter = cv2.arcLength(largest_contour, True)
        return area, perimeter
    return 0, 0

# Example usage
for category in categories:
    category_path = os.path.join(data_dir, category)
    for img_name in os.listdir(category_path):
        img_path = os.path.join(category_path, img_name)
        image = cv2.imread(img_path)
        if image is not None:
            contour_features = extract_contour_features(image)
            print(f'Contour Features for {img_name}: {contour_features}')

"""##EDGES##"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Define the dataset directory
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Path to your dataset
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']

# Function to improve edges using Canny Edge Detection
def improve_edges_canny(image):
    # Convert to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Apply Gaussian blur to reduce noise
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    # Apply Canny edge detection
    edges = cv2.Canny(blurred_image, threshold1=100, threshold2=200)
    return edges

# Function to improve edges using Unsharp Masking
def improve_edges_unsharp(image):
    # Convert to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Apply Gaussian blur
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    # Create an unsharp mask
    unsharp_mask = cv2.addWeighted(gray_image, 1.5, blurred_image, -0.5, 0)
    return unsharp_mask


for category in categories:
    category_path = os.path.join(data_dir, category)
    for img_name in os.listdir(category_path):
        img_path = os.path.join(category_path, img_name)
        image = cv2.imread(img_path)
        if image is not None:
            # Improve edges using Canny
            edges_canny = improve_edges_canny(image)
            # Improve edges using Unsharp Masking
            edges_unsharp = improve_edges_unsharp(image)

            # Display the original and processed images
            plt.figure(figsize=(12, 6))
            plt.subplot(1, 3, 1)
            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
            plt.title('Original Image')
            plt.axis('off')

            plt.subplot(1, 3, 2)
            plt.imshow(edges_canny, cmap='gray')
            plt.title('Canny Edges')
            plt.axis('off')

            plt.subplot(1, 3, 3)
            plt.imshow(edges_unsharp, cmap='gray')
            plt.title('Unsharp Masking')
            plt.axis('off')

            plt.show()

"""**IMAGE RESIZING**"""

import os
import cv2

# Define the path to your dataset and the target size for resizing
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Corrected the typo in the dataset name
output_dir = '/path/to/resized_dataset'  # Path to save resized images (optional)
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)  # Set the desired image size

# Create output directories if they do not exist
os.makedirs(output_dir, exist_ok=True)
for category in categories:
    os.makedirs(os.path.join(output_dir, category), exist_ok=True)

# Function to resize images and save them
def resize_images(data_dir, output_dir, categories, target_size):
    for category in categories:
        input_path = os.path.join(data_dir, category)
        output_path = os.path.join(output_dir, category)

        # Iterate through each image in the category folder
        for img_name in os.listdir(input_path):
            img_path = os.path.join(input_path, img_name)
            image = cv2.imread(img_path)

            if image is not None:
                # Resize the image
                resized_image = cv2.resize(image, target_size)

                # Save the resized image to the output directory
                output_image_path = os.path.join(output_path, img_name)
                cv2.imwrite(output_image_path, resized_image)

                print(f'Resized and saved: {output_image_path}')
            else:
                print(f'Failed to read image: {img_path}')

# Call the function to resize images
resize_images(data_dir, output_dir, categories, target_size)

import os
import cv2
import matplotlib.pyplot as plt

# Define the path to your dataset and the target size for resizing
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Correct dataset path
output_dir = '/content/drive/MyDrive/Resized_Pomegranate_Dataset'  # Path to save resized images
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)  # Desired image size

# Function to resize images and save them
def resize_images(data_dir, output_dir, categories, target_size):
    os.makedirs(output_dir, exist_ok=True)
    for category in categories:
        input_path = os.path.join(data_dir, category)
        output_path = os.path.join(output_dir, category)
        os.makedirs(output_path, exist_ok=True)

        # Iterate through each image in the category folder
        for img_name in os.listdir(input_path)[:10]:  # Process only 10 images
            img_path = os.path.join(input_path, img_name)
            image = cv2.imread(img_path)

            if image is not None:
                # Resize the image
                resized_image = cv2.resize(image, target_size)

                # Save the resized image to the output directory
                output_image_path = os.path.join(output_path, img_name)
                cv2.imwrite(output_image_path, resized_image)

                # Display original and resized images
                display_images(image, resized_image, category, img_name)
            else:
                print(f'Failed to read image: {img_path}')

# Function to display original and resized images side by side
def display_images(original_image, resized_image, category, img_name):
    plt.figure(figsize=(8, 4))

    # Display original image
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
    plt.title(f"Original - {category}")
    plt.axis('off')

    # Display resized image
    plt.subplot(1, 2, 2)
    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
    plt.title(f"Resized - {category}")
    plt.axis('off')

    plt.suptitle(f"{category}: {img_name}", fontsize=12)
    plt.show()

# Call the function to resize and display images
resize_images(data_dir, output_dir, categories, target_size)

"""**PREPROCESSING**"""

import os
import cv2
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define the dataset directory and target size for resizing
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Path to your dataset
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']  # Categories in the dataset
target_size = (128, 128)  # Resize all images to 128x128

# Preprocessing options
normalize = True  # Whether to normalize pixel values
convert_grayscale = False  # Convert images to grayscale

# Function to preprocess the images
def preprocess_images(data_dir, categories, target_size, normalize=True, convert_grayscale=False):
    processed_images = []  # To store the processed images
    labels = []  # To store the corresponding labels

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = categories.index(category)  # Assign numeric labels based on the category index

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)

            if image is not None:
                # Resize the image to the target size
                image = cv2.resize(image, target_size)

                # Optionally, convert to grayscale
                if convert_grayscale:
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

                # Normalize pixel values to [0, 1]
                if normalize:
                    image = image.astype('float32') / 255.0

                # Append the processed image and its label
                processed_images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    processed_images = np.array(processed_images)
    labels = np.array(labels)

    return processed_images, labels

# Call the function to preprocess images
images, labels = preprocess_images(data_dir, categories, target_size, normalize=normalize, convert_grayscale=convert_grayscale)

# Display the shape of the processed data
print(f"Processed images shape: {images.shape}")
print(f"Labels shape: {labels.shape}")

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Define the dataset directory and target size for resizing
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Path to your dataset
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']  # Categories in the dataset
target_size = (128, 128)  # Resize all images to 128x128

# Preprocessing options
normalize = True  # Whether to normalize pixel values
convert_grayscale = False  # Convert images to grayscale

# Function to preprocess the images
def preprocess_images(data_dir, categories, target_size, normalize=True, convert_grayscale=False, images_per_class=10):
    processed_images = []  # To store the processed images
    labels = []  # To store the corresponding labels

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = categories.index(category)  # Assign numeric labels based on the category index

        # Iterate through each image in the category folder and limit to 'images_per_class'
        for i, img_name in enumerate(os.listdir(category_path)):
            if i >= images_per_class:  # Limit the number of images to 'images_per_class'
                break

            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)

            if image is not None:
                # Resize the image to the target size
                image = cv2.resize(image, target_size)

                # Optionally, convert to grayscale
                if convert_grayscale:
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

                # Normalize pixel values to [0, 1]
                if normalize:
                    image = image.astype('float32') / 255.0

                # Append the processed image and its label
                processed_images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    processed_images = np.array(processed_images)
    labels = np.array(labels)

    return processed_images, labels

# Call the function to preprocess images
images, labels = preprocess_images(data_dir, categories, target_size, normalize=normalize, convert_grayscale=convert_grayscale, images_per_class=10)

# Display 10 images from each class
def display_images_by_class(images, labels, categories, images_per_class=10):
    num_classes = len(categories)
    plt.figure(figsize=(15, 15))

    for class_idx in range(num_classes):
        class_images = images[labels == class_idx]  # Filter images by class

        for i in range(images_per_class):
            plt.subplot(num_classes, images_per_class, class_idx * images_per_class + i + 1)

            # Show the image
            if images.shape[-1] == 1:  # For grayscale images
                plt.imshow(class_images[i].reshape(target_size), cmap='gray')
            else:  # For RGB images
                plt.imshow(class_images[i])

            if i == 0:
                plt.ylabel(categories[class_idx], fontsize=12)

            plt.axis('off')

    plt.tight_layout()
    plt.show()

# Display 10 images for each class
display_images_by_class(images, labels, categories, images_per_class=10)

"""##MEAN##



"""

import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# Define dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']

# Function to load images, convert them to arrays and calculate mean
def calculate_mean_per_category(data_dir, categories):
    means = []

    for category in categories:
        category_path = os.path.join(data_dir, category)
        images = []

        if os.path.exists(category_path):
            for img_file in os.listdir(category_path):
                img_path = os.path.join(category_path, img_file)
                try:
                    # Open image, convert to RGB and resize if necessary
                    img = Image.open(img_path).convert('RGB')
                    img_array = np.array(img)
                    images.append(img_array)
                except Exception as e:
                    print(f"Error loading image {img_file}: {e}")

        if images:
            images_array = np.stack(images, axis=0)
            mean_per_category = np.mean(images_array, axis=(0, 1, 2))  # Mean across height, width, channels
            means.append(mean_per_category)
        else:
            means.append(np.zeros(3))  # Default to zero if no images in category

    return means

# Calculate mean per category
mean_values = calculate_mean_per_category(data_dir, categories)

# Plot the mean values
def plot_means(categories, means):
    means = np.array(means)
    x = np.arange(len(categories))

    plt.figure(figsize=(10, 6))
    plt.bar(x - 0.3, means[:, 0], width=0.2, label='Red channel', color='r')
    plt.bar(x, means[:, 1], width=0.2, label='Green channel', color='g')
    plt.bar(x + 0.3, means[:, 2], width=0.2, label='Blue channel', color='b')

    plt.xticks(x, categories)
    plt.ylabel('Mean Pixel Value')
    plt.title('Mean Pixel Values for Each Category')
    plt.legend()
    plt.show()

# Plot the means
plot_means(categories, mean_values)

"""##

---



---



---



---



---
"""

from google.colab import drive
drive.mount('/content/drive')

"""##**CREATING MODEL USING CNN**##"""

import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

# Set dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)

# Preprocess function (load, resize, normalize, and label the images)
def preprocess_images(data_dir, categories, target_size):
    images = []
    labels = []

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = category  # Use category names as labels

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            if image is not None:
                # Resize image
                image = cv2.resize(image, target_size)
                # Normalize image to [0, 1]
                image = image.astype('float32') / 255.0
                # Append image and label
                images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    images = np.array(images)
    labels = np.array(labels)

    return images, labels

# Load and preprocess images
images, labels = preprocess_images(data_dir, categories, target_size)

# One-hot encode the labels
lb = LabelBinarizer()
labels = lb.fit_transform(labels)

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Define a CNN model
def create_cnn_model(input_shape, num_classes):
    model = Sequential()

    # 1st Convolutional Layer
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # 2nd Convolutional Layer
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # 3rd Convolutional Layer
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Flatten the output
    model.add(Flatten())

    # Fully Connected Layer
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))  # Dropout to reduce overfitting

    # Output Layer (Softmax for multi-class classification)
    model.add(Dense(num_classes, activation='softmax'))

    return model

# Create the model
input_shape = (target_size[0], target_size[1], 3)  # 3 channels for RGB images
num_classes = len(categories)  # Number of categories/classes
model = create_cnn_model(input_shape, num_classes)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy:.4f}')

# Save the model (optional)
model.save('image_classification_model.h5')

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Generate predictions on the test set
y_pred = model.predict(X_test)

# Convert predictions and true labels from one-hot encoding to class indices
y_pred_classes = np.argmax(y_pred, axis=1)  # Predicted class labels
y_true = np.argmax(y_test, axis=1)          # True class labels

# Confusion Matrix
conf_matrix = confusion_matrix(y_true, y_pred_classes)

# Plot the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=categories, yticklabels=categories, fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
class_report = classification_report(y_true, y_pred_classes, target_names=categories)
print("Classification Report:")
print(class_report)

import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
import matplotlib.pyplot as plt

# Set dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)

# Preprocess function (load, resize, normalize, and label the images)
def preprocess_images(data_dir, categories, target_size):
    images = []
    labels = []

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = category  # Use category names as labels

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            if image is not None:
                # Resize image
                image = cv2.resize(image, target_size)
                # Normalize image to [0, 1]
                image = image.astype('float32') / 255.0
                # Append image and label
                images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    images = np.array(images)
    labels = np.array(labels)

    return images, labels

# Load and preprocess images
images, labels = preprocess_images(data_dir, categories, target_size)

# One-hot encode the labels
lb = LabelBinarizer()
labels = lb.fit_transform(labels)

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Data augmentation to prevent overfitting
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

# Define a CNN model
def create_cnn_model(input_shape, num_classes):
    model = Sequential()

    # 1st Convolutional Layer
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # 2nd Convolutional Layer
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # 3rd Convolutional Layer
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Flatten the output
    model.add(Flatten())

    # Fully Connected Layer
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))  # Dropout to reduce overfitting

    # Output Layer (Softmax for multi-class classification)
    model.add(Dense(num_classes, activation='softmax'))

    return model

# Create the model
input_shape = (target_size[0], target_size[1], 3)  # 3 channels for RGB images
num_classes = len(categories)  # Number of categories/classes
model = create_cnn_model(input_shape, num_classes)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks: Save best model and reduce learning rate on plateau
callbacks = [
    tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss'), # Changed 'best_model.h5' to 'best_model.keras'
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)
]

# Train the model with data augmentation
history = model.fit(datagen.flow(X_train, y_train, batch_size=32),
                    epochs=20, validation_data=(X_test, y_test),
                    callbacks=callbacks)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy:.4f}')

# Save the model
model.save('image_classification_model.h5')

# Plot training & validation accuracy/loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss')

plt.show()

"""---



---



---



---



---



---

##**CREATE MODEL USING KNN**##

**ACCURACY AND LOSS GRAPH**
"""

import matplotlib.pyplot as plt

# Plot accuracy graph
def plot_accuracy(history):
    plt.figure(figsize=(8, 6))
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

# Plot loss graph
def plot_loss(history):
    plt.figure(figsize=(8, 6))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend(loc='upper right')
    plt.grid(True)
    plt.show()

# Call the functions to plot accuracy and loss
plot_accuracy(history)
plot_loss(history)

"""##**SAVE THE KNN MODEL**##"""

import os
import numpy as np
import cv2
import joblib  # To save and load the model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelBinarizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score

# Set dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)

# Preprocess function (load, resize, normalize, and label the images)
def preprocess_images(data_dir, categories, target_size):
    images = []
    labels = []

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = category  # Use category names as labels

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            if image is not None:
                # Resize image
                image = cv2.resize(image, target_size)
                # Flatten the image into a 1D array for KNN
                image = image.flatten()
                # Append image and label
                images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    images = np.array(images)
    labels = np.array(labels)

    return images, labels

# Load and preprocess images
images, labels = preprocess_images(data_dir, categories, target_size)

# One-hot encode the labels (for consistency)
lb = LabelBinarizer()
labels = lb.fit_transform(labels)

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Convert one-hot encoded labels back to single labels for KNN
y_train = np.argmax(y_train, axis=1)
y_test = np.argmax(y_test, axis=1)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA for dimensionality reduction
pca = PCA(n_components=100)  # Reduce to 100 components
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_neighbors': np.arange(1, 20),
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

knn = KNeighborsClassifier()

# Perform grid search
grid_search = GridSearchCV(knn, param_grid, cv=5)
grid_search.fit(X_train_pca, y_train)

# Best hyperparameters
best_n_neighbors = grid_search.best_params_['n_neighbors']
best_metric = grid_search.best_params_['metric']
print(f'Best n_neighbors: {best_n_neighbors}')
print(f'Best distance metric: {best_metric}')

# Train final KNN model with best parameters
best_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors, metric=best_metric)
best_knn.fit(X_train_pca, y_train)

# Predict and evaluate on the test set
y_pred = best_knn.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)
print(f'Test Accuracy: {accuracy:.4f}')

# Save the model and preprocessing objects (scaler, PCA)
joblib.dump(best_knn, 'knn_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(pca, 'pca.pkl')

print("Model and preprocessing objects saved successfully.")

import cv2
import numpy as np
import joblib

# Load the pre-trained model, scaler, and PCA
knn_model = joblib.load('knn_model.pkl')
scaler = joblib.load('scaler.pkl')
pca = joblib.load('pca.pkl')

# Define the categories
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)  # Resize to 128x128 as per the training process

def preprocess_image(img_path, target_size):
    """
    Preprocess the input image for prediction.
    - Resize to the target size.
    - Flatten the image.
    - Normalize to the [0, 1] range.
    """
    # Read the image
    image = cv2.imread(img_path)
    if image is None:
        raise ValueError(f"Image not found at path: {img_path}")

    # Resize the image
    image = cv2.resize(image, target_size)

    # Flatten and normalize
    image = image.astype('float32') / 255.0
    image = image.flatten()

    return image

def predict_image(img_path):
    """
    Predict whether the image is healthy or diseased.
    """
    try:
        # Preprocess the input image
        image = preprocess_image(img_path, target_size)

        # Scale the image features
        image_scaled = scaler.transform([image])

        # Reduce dimensions with PCA
        image_pca = pca.transform(image_scaled)

        # Predict with the KNN model
        prediction = knn_model.predict(image_pca)
        predicted_category = categories[prediction[0]]

        return predicted_category

    except Exception as e:
        return str(e)

# Get input from the user
image_path = input("Enter the path of the image to predict: ").strip()

# Predict and display the result
result = predict_image(image_path)
print(f"The predicted category is: {result}")

from sklearn.metrics import classification_report

# Predict on the test set
y_pred = best_knn.predict(X_test_pca)

# Classification report for the test set
print("Classification Report (Test Set):")
print(classification_report(y_test, y_pred, target_names=categories))

# Predict on the training set
y_train_pred = best_knn.predict(X_train_pca)

# Classification report for the training set
print("Classification Report (Training Set):")
print(classification_report(y_train, y_train_pred, target_names=categories))

from sklearn.metrics import confusion_matrix

# Predict on the test set
y_pred = best_knn.predict(X_test_pca)

# Confusion matrix for the test set
print("Confusion Matrix (Test Set):")
print(confusion_matrix(y_test, y_pred))

# Predict on the training set
y_train_pred = best_knn.predict(X_train_pca)

# Confusion matrix for the training set
print("Confusion Matrix (Training Set):")
print(confusion_matrix(y_train, y_train_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using seaborn's heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=lb.classes_, yticklabels=lb.classes_) # Use lb.classes_ instead of label_encoder.classes_
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

"""##Accuracy and loss graph##"""

import matplotlib.pyplot as plt

# Arrays to store accuracies and errors
neighbors_range = np.arange(1, 20)
train_accuracies = []
test_accuracies = []
train_errors = []
test_errors = []

# Loop over different values of k (number of neighbors)
for k in neighbors_range:
    knn = KNeighborsClassifier(n_neighbors=k, metric=best_metric)
    knn.fit(X_train_pca, y_train)

    # Training accuracy
    y_train_pred = knn.predict(X_train_pca)
    train_acc = accuracy_score(y_train, y_train_pred)
    train_accuracies.append(train_acc)
    train_errors.append(1 - train_acc)  # Error = 1 - accuracy

    # Test accuracy
    y_test_pred = knn.predict(X_test_pca)
    test_acc = accuracy_score(y_test, y_test_pred)
    test_accuracies.append(test_acc)
    test_errors.append(1 - test_acc)  # Error = 1 - accuracy

# Plotting the accuracy
plt.figure(figsize=(12, 6))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(neighbors_range, train_accuracies, label='Train Accuracy', marker='o')
plt.plot(neighbors_range, test_accuracies, label='Test Accuracy', marker='o')
plt.title('Accuracy vs. Number of Neighbors')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.legend()

# Error Rate Plot
plt.subplot(1, 2, 2)
plt.plot(neighbors_range, train_errors, label='Train Error', marker='o')
plt.plot(neighbors_range, test_errors, label='Test Error', marker='o')
plt.title('Error Rate vs. Number of Neighbors')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Error Rate')
plt.legend()

plt.tight_layout()
plt.show()

""",

---



---



---



---



---



---



---

##SVM MODEL##
"""

import os
import numpy as np
import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import seaborn as sns

# Set dataset directory and categories
data_dir = '/content/drive/MyDrive/Pomegranate Diseases Dataset'  # Path to your dataset
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']
target_size = (128, 128)  # Resize images to a uniform size

# Function to preprocess and flatten images
def preprocess_images(data_dir, categories, target_size):
    images = []
    labels = []

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = category  # Use category names as labels

        # Iterate through each image in the category folder
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            image = cv2.imread(img_path)
            if image is not None:
                # Resize image to target size
                image = cv2.resize(image, target_size)
                # Flatten the image into a 1D array
                image = image.flatten()
                # Normalize pixel values to [0, 1]
                image = image.astype('float32') / 255.0
                # Append image and label
                images.append(image)
                labels.append(label)
            else:
                print(f'Failed to process image: {img_path}')

    # Convert lists to NumPy arrays
    images = np.array(images)
    labels = np.array(labels)

    return images, labels

# Preprocess the dataset
images, labels = preprocess_images(data_dir, categories, target_size)

# Encode the labels as integers
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Create an SVM classifier model (RBF kernel by default)
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')  # You can experiment with 'linear', 'poly', or 'rbf'

# Train the SVM model
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Test Accuracy: {accuracy:.4f}')

# Detailed classification report
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

"""##ACCURACY AND LOSS GRAPH##

"""

import matplotlib.pyplot as plt

# Plot accuracy over epochs
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Plot loss over epochs
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='upper right')

# Show the plots
plt.tight_layout()
plt.show()

"""##**ROC CURVE**##"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC

# Assuming you have y_test and y_score from your model

# Binarize the labels
y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4])  # Adjust for your number of classes

# Use decision_function or predict_proba to get y_score
y_score = svm_model.decision_function(X_test)

# Compute ROC curve and ROC area for each class
fpr = {}
tpr = {}
roc_auc = {}
for i in range(len([0, 1, 2, 3, 4])):  # Adjust for your number of classes
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve for each class
plt.figure(figsize=(10, 8))
for i in range(len([0, 1, 2, 3, 4])):  # Adjust for your number of classes
    plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic for Multi-class')
plt.legend(loc='lower right')
plt.show()

from google.colab import drive
drive.mount('/content/drive')

import cv2
import numpy as np

def preprocess_user_image(img_path, target_size):
    """
    Preprocesses the input image for SVM prediction.
    Args:
        img_path (str): Path to the input image.
        target_size (tuple): Size to which the image should be resized.
    Returns:
        np.array: Flattened and normalized image ready for prediction.
    """
    image = cv2.imread(img_path)
    if image is not None:
        # Resize image to match training preprocessing
        image = cv2.resize(image, target_size)
        # Flatten the image into a 1D array
        image = image.flatten()
        # Normalize pixel values to [0, 1]
        image = image.astype('float32') / 255.0
        return image
    else:
        raise FileNotFoundError(f"Image at path {img_path} not found.")

# Prompt user for image path
img_path = input("Enter the path to the image: ")

try:
    # Preprocess the user-provided image
    processed_image = preprocess_user_image(img_path, target_size)
    processed_image = processed_image.reshape(1, -1)  # Reshape for SVM input

    # Predict using the trained SVM model
    prediction = svm_model.predict(processed_image)[0]

    # Decode the prediction
    predicted_label = label_encoder.inverse_transform([prediction])[0]

    # Check if the prediction is healthy or diseased
    if predicted_label == 'Healthy':
        print("The image is classified as Healthy.")
    else:
        print(f"The image is classified as Diseased: {predicted_label}")
except Exception as e:
    print(f"Error: {e}")

import os
import cv2
import numpy as np
import tensorflow as tf

# Load the trained model
model = tf.keras.models.load_model('image_classification_model.h5')

# Define the categories (diseases)
categories = ['Healthy', 'Anthracnose', 'Bacterial_Blight', 'Cercospora', 'Alternaria']

# Function to preprocess the input image
def preprocess_image(image_path):
    image = cv2.imread(image_path)
    image = cv2.resize(image, (128, 128))  # Resize to match model input size
    image = image.astype('float32') / 255.0  # Normalize pixel values
    image = np.expand_dims(image, axis=0)  # Add batch dimension
    return image

# Function to predict the disease
def predict_disease(image_path):
    processed_image = preprocess_image(image_path)
    prediction = model.predict(processed_image)
    predicted_class_index = np.argmax(prediction)
    predicted_disease = categories[predicted_class_index]
    return predicted_disease

# Example usage:
image_path = '/path/to/your/image.jpg'  # Replace with the actual path to your image
predicted_disease = predict_disease(image_path)
print(f"Predicted Disease: {predicted_disease}")